import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import RandomizedSearchCV, KFold, train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.utils import resample
from scipy.stats import uniform, randint
import matplotlib.pyplot as plt

plt.rcParams['font.sans-serif'] = ['SimHei']  # 解决中文乱码
plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题

# 读取数据
data = pd.read_csv("历史0319.csv", encoding="utf-8")
cols = ['PTCD', 'RIQ', 'DNBS', 'DNMS', 'A', 'FD', 'SCR', 'MBA', 'MBH', 'BD',
        'FAR', 'RPOI', 'EPOI', 'SPOI', 'HP', 'POP']
X = data[cols]
y = data['NV']

# 对目标变量进行对数变换，提高线性关系
y_transformed = np.log1p(y)

# 划分训练集和测试集（80% 训练，20% 测试）
X_train, X_test, y_train, y_test = train_test_split(X, y_transformed, test_size=0.2, random_state=42)

# 定义 XGBoost 回归模型
model = xgb.XGBRegressor(objective="reg:squarederror", eval_metric="rmse", random_state=42)

# 定义超参数搜索空间
param_dist = {
    'n_estimators': randint(100, 800),
    'max_depth': randint(3, 5),
    'learning_rate': uniform(0.001, 0.1),
    'subsample': uniform(0.7, 0.2),
    'colsample_bytree': uniform(0.6, 0.4),
    'reg_alpha': uniform(0.1, 0.5),
    'reg_lambda': uniform(1, 5),
}

# K 折交叉验证（K=10）
kf = KFold(n_splits=10, shuffle=True, random_state=42)

# 进行随机搜索 + 交叉验证
random_search = RandomizedSearchCV(
    model,
    param_distributions=param_dist,
    n_iter=20,
    cv=kf,
    scoring='r2',
    n_jobs=-1,
    random_state=42
)
random_search.fit(X_train, y_train)

# 输出最佳参数和最佳 R² 分数
best_params = random_search.best_params_
best_r2_train = random_search.best_score_
print("最佳参数:", best_params)
print("交叉验证 R² 均值:", best_r2_train)

# 训练最佳模型
best_model = xgb.XGBRegressor(**best_params)
best_model.fit(X_train, y_train)

# 预测测试集
y_test_pred_transformed = best_model.predict(X_test)
y_test_pred = np.expm1(y_test_pred_transformed)  # 逆变换
y_test_true = np.expm1(y_test)

# 计算测试集评估指标
test_r2 = r2_score(y_test_true, y_test_pred)
test_mse = mean_squared_error(y_test_true, y_test_pred)
test_mae = mean_absolute_error(y_test_true, y_test_pred)
test_rmse = np.sqrt(test_mse)

print("测试集 R²:", test_r2)
print("测试集 MSE:", test_mse)
print("测试集 MAE:", test_mae)
print("测试集 RMSE:", test_rmse)


# 计算交叉验证 R² 置信区间（Bootstrap 方法）
def bootstrap_cv_r2(X, y, model, kfolds, n_iterations=1000, confidence_level=95):
    r2_scores = []

    for _ in range(n_iterations):
        X_resampled, y_resampled = resample(X, y)
        kf = KFold(n_splits=kfolds, shuffle=True, random_state=None)

        r2_fold_scores = []
        for train_idx, val_idx in kf.split(X_resampled):
            X_train_cv, X_val_cv = X_resampled.iloc[train_idx], X_resampled.iloc[val_idx]
            y_train_cv, y_val_cv = y_resampled.iloc[train_idx], y_resampled.iloc[val_idx]

            model_cv = xgb.XGBRegressor(**best_params)
            model_cv.fit(X_train_cv, y_train_cv)
            y_val_pred = model_cv.predict(X_val_cv)
            r2_fold_scores.append(r2_score(y_val_cv, y_val_pred))

        r2_scores.append(np.mean(r2_fold_scores))

    lower_bound = np.percentile(r2_scores, (100 - confidence_level) / 2)
    upper_bound = np.percentile(r2_scores, 100 - (100 - confidence_level) / 2)
    return lower_bound, upper_bound, r2_scores


# 计算交叉验证 R² 置信区间
cv_lower, cv_upper, r2_cv_bootstrap = bootstrap_cv_r2(X_train, y_train, best_model, kfolds=10)
print(f"交叉验证 R² 置信区间 (Bootstrap, 95%): ({cv_lower:.4f}, {cv_upper:.4f})")

# 绘制 Bootstrap 交叉验证 R² 分布
plt.figure(figsize=(8, 5))
plt.hist(r2_cv_bootstrap, bins=30, alpha=0.7, color='blue', edgecolor='black')
plt.axvline(cv_lower, color='red', linestyle='dashed', label=f"Lower Bound: {cv_lower:.4f}")
plt.axvline(cv_upper, color='green', linestyle='dashed', label=f"Upper Bound: {cv_upper:.4f}")
plt.xlabel("Cross-Validation R² Score")
plt.ylabel("Frequency")
plt.title("Bootstrap Cross-Validation R² Score Distribution")
plt.legend()
plt.show()
