import pandas as pd
import xgboost as xgb
from sklearn.model_selection import RandomizedSearchCV, KFold, train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from scipy.stats import uniform, randint
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import shap
from statsmodels.nonparametric.smoothers_lowess import lowess

plt.rcParams['font.sans-serif'] = ['SimHei']  # ä½¿ç”¨é»‘ä½“
plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# è¯»å–æ•°æ®
data = pd.read_csv("urban.csv", encoding="utf-8")
cols = ['PTCD', 'RIQ', 'DNBS', 'DNMS', 'A', 'FD', 'SCR', 'MBA', 'MBH', 'BD', 'FAR', 'RPOI', 'EPOI', 'SPOI', 'HP', 'POP']
X = data[cols]
y = data['å¤§ä¼—å½’ä¸€']

# å˜æ¢ yï¼Œæé«˜çº¿æ€§å…³ç³»ï¼ˆå¯¹æ•°å˜æ¢ï¼‰
y_transformed = np.log1p(y)

# åˆ’åˆ†æ•°æ®é›†ï¼ˆ80% è®­ç»ƒé›†ï¼Œ20% æµ‹è¯•é›†ï¼‰
X_train, X_test, y_train_transformed, y_test_transformed = train_test_split(X, y_transformed, test_size=0.2, random_state=42)

# ä»è®­ç»ƒé›†ä¸­å†åˆ’åˆ† 10% ä½œä¸ºéªŒè¯é›†
X_train_sub, X_valid, y_train_sub, y_valid = train_test_split(X_train, y_train_transformed, test_size=0.1, random_state=42)

# å®šä¹‰ XGBoost å›å½’æ¨¡å‹
model = xgb.XGBRegressor(objective="reg:squarederror", eval_metric="rmse", random_state=42)

# å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´

# å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´
param_dist = {'n_estimators': randint(100, 800),
              'max_depth': randint(3, 5),
              'learning_rate': uniform(0.001, 0.1),    'subsample': uniform(0.7, 0.2),
              'colsample_bytree': uniform(0.6, 0.4),    'reg_alpha': uniform(2, 10),
              'reg_lambda': uniform(1, 5),}




# K æŠ˜äº¤å‰éªŒè¯ï¼ˆK=10ï¼‰
kf = KFold(n_splits=10, shuffle=True, random_state=42)

# è¿›è¡Œéšæœºæœç´¢ + äº¤å‰éªŒè¯
random_search = RandomizedSearchCV(
    model,
    param_distributions=param_dist,
    n_iter=20,
    cv=kf,
    scoring='r2',
    n_jobs=-1,
    random_state=42
)

# è®­ç»ƒæ¨¡å‹å¹¶æ·»åŠ æ—©åœ
random_search.fit(
    X_train_sub, y_train_sub,
    eval_set=[(X_valid, y_valid)],  # æ—©åœä½¿ç”¨éªŒè¯é›†
    early_stopping_rounds=20,  # å¦‚æœè¿ç»­ 20 è½®æ— æå‡ï¼Œåˆ™åœæ­¢è®­ç»ƒ
    verbose=True
)

# è¾“å‡ºæœ€ä½³å‚æ•°å’Œæœ€ä½³ RÂ² åˆ†æ•°
print("æœ€ä½³å‚æ•°:", random_search.best_params_)
print("æœ€ä½³ RÂ²:", random_search.best_score_)

# è·å–æœ€ç»ˆçš„è®­ç»ƒç»“æœ
best_model = random_search.best_estimator_

# åœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œé¢„æµ‹
y_train_pred_transformed = best_model.predict(X_train)

# åå˜æ¢é¢„æµ‹å€¼
y_train_pred = np.expm1(y_train_pred_transformed)
y_train_true = np.expm1(y_train_transformed)

# è®¡ç®—è®­ç»ƒé›†ä¸Šçš„è¯„ä¼°æŒ‡æ ‡
train_r2 = r2_score(y_train_true, y_train_pred)
train_mse = mean_squared_error(y_train_true, y_train_pred)
train_mae = mean_absolute_error(y_train_true, y_train_pred)
train_rmse = np.sqrt(train_mse)

# è¾“å‡ºè®­ç»ƒé›†ä¸Šçš„è¯„ä¼°ç»“æœ
print("è®­ç»ƒé›†ä¸Šçš„ RÂ²:", train_r2)
print("è®­ç»ƒé›†ä¸Šçš„ MSE:", train_mse)
print("è®­ç»ƒé›†ä¸Šçš„ MAE:", train_mae)
print("è®­ç»ƒé›†ä¸Šçš„ RMSE:", train_rmse)

# åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œé¢„æµ‹
y_pred_transformed = best_model.predict(X_test)

# åå˜æ¢é¢„æµ‹å€¼
y_pred = np.expm1(y_pred_transformed)
y_true = np.expm1(y_test_transformed)

# è®¡ç®—æµ‹è¯•é›†ä¸Šçš„è¯„ä¼°æŒ‡æ ‡
final_r2 = r2_score(y_true, y_pred)
final_mse = mean_squared_error(y_true, y_pred)
final_mae = mean_absolute_error(y_true, y_pred)
final_rmse = np.sqrt(final_mse)

# è¾“å‡ºæµ‹è¯•é›†ä¸Šçš„è¯„ä¼°ç»“æœ
print("æµ‹è¯•é›†ä¸Šçš„æœ€ç»ˆ RÂ²:", final_r2)
print("æµ‹è¯•é›†ä¸Šçš„æœ€ç»ˆ MSE:", final_mse)
print("æµ‹è¯•é›†ä¸Šçš„æœ€ç»ˆ MAE:", final_mae)
print("æµ‹è¯•é›†ä¸Šçš„æœ€ç»ˆ RMSE:", final_rmse)

# ------------------ ğŸ”¹ SHAP è§£é‡Š ------------------
#explainer = shap.TreeExplainer(best_model)
#shap_values = explainer.shap_values(X)
#print(shap_values.shape)


# SHAP è®¡ç®—
explainer = shap.TreeExplainer(best_model)
shap_values = explainer.shap_values(X)

# åå˜æ¢ SHAP è§£é‡Šå€¼ï¼Œä½¿å…¶ä¸ y_pred å¤„äºç›¸åŒå°ºåº¦
shap_values_expm1 = np.expm1(shap_values)

# SHAP summary plot
shap.summary_plot(shap_values_expm1, X)



# é€‰æ‹©ä¸€ä¸ªæ ·æœ¬è¿›è¡Œè§£é‡Š
j = 13
player_explainer = pd.DataFrame()
player_explainer['feature'] = cols
player_explainer['feature_value'] = X.iloc[j].values
player_explainer['shap_value'] = shap_values[j]
print(player_explainer)

shap.initjs()

# SHAP summary plot
plt.figure(facecolor='white')
shap.summary_plot(shap_values, X, show=False)
ax = plt.gca()
ax.set_facecolor('white')
ax.grid(True, axis='x', linestyle='--', linewidth=0.3, color='gray')
ax.spines['bottom'].set_linewidth(1)
ax.spines['left'].set_linewidth(1)
ax.spines['top'].set_linewidth(1)
ax.spines['right'].set_linewidth(1)
ax.spines['bottom'].set_color('black')
ax.spines['left'].set_color('black')
ax.spines['top'].set_color('white')
ax.spines['right'].set_color('white')
ax.xaxis.label.set_color('black')
ax.yaxis.label.set_color('black')
ax.tick_params(axis='x', colors='black')
ax.tick_params(axis='y', colors='black')
plt.show()

# SHAP dependence plot
plt.figure(facecolor='white')
shap.dependence_plot('RIQ', shap_values, X, interaction_index=None, show=False)
#shap.dependence_plot('EPOI', shap_values, data[cols], interaction_index='EPOI', show=False)
ax = plt.gca()
ax.set_facecolor('white')
ax.spines['bottom'].set_linewidth(1)
ax.spines['left'].set_linewidth(1)
ax.spines['top'].set_linewidth(1)
ax.spines['right'].set_linewidth(1)
ax.spines['bottom'].set_color('black')
ax.spines['left'].set_color('black')
ax.spines['top'].set_color('white')
ax.spines['right'].set_color('white')
ax.xaxis.label.set_color('black')
ax.yaxis.label.set_color('black')
ax.tick_params(axis='x', colors='black')
ax.tick_params(axis='y', colors='black')
plt.axhline(y=0, color='black', linestyle='--', linewidth=0.5)

# æ·»åŠ æ‹Ÿåˆæ›²çº¿
smoothed = lowess(shap_values[:, cols.index('RIQ')], X['RIQ'], frac=0.2)
plt.plot(smoothed[:, 0], smoothed[:, 1], color='orange', linewidth=2)
ax.set_ylabel('SHAP Value', color='black', fontsize=12)
plt.show()

fig, ax1 = plt.subplots(figsize=(12, 8), dpi=100)  # è°ƒæ•´å›¾å½¢å¤§å°

# Bee Swarm plot
shap.summary_plot(shap_values, X, feature_names=X.columns, plot_type="dot", show=False, color_bar=True)

ax1 = plt.gca()

# åˆ›å»ºç¬¬äºŒä¸ªåæ ‡è½´
ax2 = ax1.twiny()

# Feature Importance plot
shap.summary_plot(shap_values, X, plot_type="bar", show=False)

# è°ƒæ•´å›¾å½¢ä½ç½®ï¼Œé¿å…é‡å 
plt.gca().set_position([0.1, 0.5, 0.8, 0.45])  # é€‚å½“è°ƒæ•´ä½ç½®

# æ·»åŠ æ°´å¹³çº¿ä»¥ä¾¿å‚è€ƒ
ax2.axhline(y=13, color='gray', linestyle='-', linewidth=1)

# ä¿®æ”¹æ¡å½¢å›¾é€æ˜åº¦
bars = ax2.patches
for bar in bars:
    bar.set_alpha(0.4)  # è°ƒæ•´é€æ˜åº¦

# è®¾ç½®æ ‡ç­¾å’Œå­—ä½“å¤§å°
ax1.set_xlabel('Shapley Value Contribution (Bee Swarm)', fontsize=12)
ax2.set_xlabel('Mean Shapley Value (Feature Importance)', fontsize=12)
ax2.xaxis.set_label_position('top')
ax2.xaxis.tick_top()
ax1.set_ylabel('Features', fontsize=12)

# é€‚å½“è°ƒæ•´å­—ä½“å¤§å°
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)

# è°ƒæ•´æ•´ä½“å¸ƒå±€ï¼Œä»¥é˜²æ­¢æ ‡ç­¾é‡å 
plt.tight_layout()

# ä¿å­˜å›¾åƒ
plt.savefig("SHAP_combined_with_top_line_corrected.pdf", format='pdf', bbox_inches='tight')
plt.show()
